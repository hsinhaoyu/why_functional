#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
#+EXPORT_FILE_NAME: ../html/diff.html
#+OPTIONS: broken-links:t
#+TITLE: Numerical differentiation using generators
From a glance, this section of Hughes' paper appears to be all about numerical methods that are only of interest to the readers of [[http://numerical.recipes][Numerical Recipes]]. But do read on. This chapter demonstrates several interesting tricks that you can use with generators.

* A first stab at the problem
For a real function =f=, the first-order approximation of =f'= at =x= can be obtained by displacing =x= by =h=, and then calculating the slope:
#+begin_src python :noweb no-export :tangle ../src/diff.py
  <<DIFF_IMPORTS>>

  def easydiff(f: Callable[[float], float], x: float) -> Callable[[float], float]:
      def easydiff_(h: float) -> float:
          return (f(x + h) - f(x)) / h

      return easydiff_
#+end_src

Now we generate an iterator of smaller and smaller =h=, by applying =half= repeatedly with =repeat_f= (defined in =newton.org=). Mapping the slope function to this iterator, =differentiate= returns another iterator that approximates the derivative closer and closer.
#+begin_src python :noweb yes :tangle ../src/diff.py
  def half(x: float) -> float:
      """Half the value"""
      return x / 2.0

  def differentiate(h0: float, f: Callable[[float], float], x: float) -> Iterator[float]:
      """An interator of 1st-order approximation of f'(x) with initial h0"""
      return map(easydiff(f, x), repeat_f(half, h0))
#+end_src

This is our first version of the numerical differentiation function:
#+begin_src python :noweb yes :tangle ../src/diff.py
  def diff1(h0: float, f: Callable[[float], float], x: float) -> float:
      """Approximate f'(x), with an initial h0."""
      d = within(esp, differentiate(h0, f, x))
      return next(d)
#+end_src

A simple test: differentiate sin(x) at 1.0, starting with a very large gap (=h0=5.0=). We know that the right answer is cos(1.0).
#+begin_src python :noweb no-export :tangle ../src/test_diff.py
  <<TEST_DIFF_IMPORTS>>

  def f(x: float) -> float:
      return sin(x)

  def test_diff1():
      h0, x = 5.0, 1.0
      d = diff1(h0, f, x)
      assert d == pytest.approx(cos(x))
#+end_src

* Improve convergence with sequence transformation
A problem with =differentiate= above is that the sequence converges slowly. This sections uses a numerical trick to improve the convergence. We'll focus on the code structure, not the math, but the technique is called [[https://en.wikipedia.org/wiki/Richardson_extrapolation][Richardson extrapolation]]. It's a sequence transformation technique that can be use on a wide range of approximating sequences. We'll use it again in the [[org/integration.org][next chapter]].

With an appropriately-chosen integer =n=, the convergence of a sequence (represented as a generator) can improved by the =elimerror= function defined below. What it does is that it slides a length 2 window over the original sequence, and applies a correction based on the two values(=c= below). 
#+begin_src python :noweb yes :tangle ../src/diff.py
  def elimerror(n: int, itr: Iterator[float]) -> Iterator[float]:
      """Reduce the error of sequence approx. derivative, assuming order n."""
      a = next(itr)
      while True:
          b = next(itr)
          p = 2.0 ** n
          c = (b * p - a) / (p - 1.0)
          yield c
          a = b
#+end_src

We don't know how to set =n= yet, but let's try the code with an arbitrary number (=n=2=). 
#+begin_src python :noweb yes :tangle ../src/test_diff.py :results output
  def test_elimerror():
      print("\n## test_elimerror:")

      h0, x, n = 5.0, 1.0, 2

      seq1 = differentiate(h0, f, x)
      print("seq1:", list(islice(seq1, 20)))

      seq1 = differentiate(h0, f, x)     
      seq2 = elimerror(n, seq1)

      print("seq2:", list(islice(seq2, 20)))
#+end_src

The appropriate =n= can be estimated by =order=, using the first three values in the sequence. Taking a slow converging iterator as input, =improve= returns a new iterator that converges faster. It does it by estimating the order and calling =elimerror=.
#+begin_src python :noweb yes :tangle ../src/diff.py
  def order(itr: Iterator[float]) -> int:
      """Estimate the order for elimerror()."""
      a, b, c = next(itr), next(itr), next(itr)
      return round(log2((a - c) / (b - c) - 1.0))

  def improve(itr: Iterator[float]) -> Iterator[float]:
      """Improve the congergence of sequence approx. derivative."""
      (itr1, itr2) = tee(itr)
      n: int = order(itr1)
      return elimerror(n, itr2)
#+end_src

You can compare the two sequences by running =make test=:
#+begin_src python :noweb yes :tangle ../src/test_diff.py
  def test_improve():
      print("\n## test_improve")

      h0, x = 2.0, 0.3
      
      seq1 = differentiate(h0, f, x)
      print("seq1:", list(islice(seq1, 20)))

      seq1 = differentiate(h0, f, x)
      seq2 = improve(seq1)

      print("seq2:", list(islice(seq2, 20)))
#+end_src

This is second version of the numerical differentiation function. The logic is very easy to understand, because the code is written as a chain of simple functions: generate a series of rough calculations, improve them, and stop when the convergence criterion is met (=esp= is a small number defined in the Appendix). 
#+begin_src python :noweb yes :tangle ../src/diff.py
  def diff2(h0: float, f: Callable[[float], float], x: float) -> float:
      """Approximate f'(x), with an initial h0."""
      d = within(esp, improve(differentiate(h0, f, x)))
      return next(d)
#+end_src

A simple test:
#+begin_src python :noweb yes :tangle ../src/test_diff.py
  def test_diff2():
      h0, x = 1.0, 0.3
      d = diff2(h0, f, x)
      assert d == pytest.approx(cos(x))
#+end_src

* An iterator of iterators
Do we actually need further improvements on the convergence? Not for the simple problems that we've been solving, but this is where the code gets interesting!

In Hughes' paper, he used the =improve= function again and again on the same sequence to get better and better convergence. Let =s= be the infinite iterator returned by =differentiate(f0, f x)=. By calling =repeat(improve, s)=, we get =s=, =improve(s)=, =improve(improve(s))=... and so on. It's an infinite iterator of infinite iterators!

The paper expresses this idea with a beautiful one-liner. Unfortunately, Python's iterator is not as elegant. The =repeat_f= function defined [[org/newton.org][previously]] doesn't work on iterators, so we'll need a specialized version:
#+begin_src python :noweb yes :tangle ../src/lazy_utils.py
  def repeat_itr(f: Callable[[Iterator], Iterator], i: Iterator) -> Iterator:
      """[i, f(i), f(f(i))...]"""
      acc: Iterator[float] = i

      while True:
          (i0, i1) = tee(acc)
          yield i0
          acc = f(i1)
#+end_src

Let's see if the 5th item in the yielded iterator is the same as applying =improve= 4 times:
#+begin_src python :noweb yes :tangle ../src/test_diff.py
  def test_repeat_improve():
      print("\n## test_repeat_improve():")
      def f(x):
          return sin(x)

      d = differentiate(1.0, f, 0.3)
      d4 = improve(improve(improve(improve(d))))
      seq1 = list(islice(d4, 5))
      print("seq1:", seq1)

      d = differentiate(1.0, f, 0.3)
      dx = repeat_itr(improve, d)
      next(dx)
      next(dx)
      next(dx)
      next(dx)
      seq2 = list(islice(next(dx), 5))
      print("seq2:", seq2)

      assert seq1 == seq2
#+end_src

I haven't checked a textbook, but the paper says that the second approximation from each of the improved iterator is a good sequence for numerical differentiation. So, this is the third version of the numerical differentiation function:
#+begin_src python :noweb yes :tangle ../src/diff.py
  def second(itr: Iterator[float]) -> float:
      """Returns the second item in an iterator."""
      next(itr)
      return next(itr)

  def super_improve(itr: Iterator[float]) -> Iterator[float]:
      """Improve the convergenve of a sequence approx. derivative."""
      return map(second, repeat_itr(improve, itr))

  def diff3(h0: float, f: Callable[[float], float], x: float) -> float:
      """Approximate f'(x), with an initial h0."""      
      d = within(esp, super_improve(differentiate(h0, f, x)))
      return next(d)
#+end_src

Does it work?
#+begin_src python :noweb yes :tangle ../src/test_diff.py
  def test_diff3():
      h0, x = 1.0, 0.3
      d = diff3(h0, f, x)
      assert d == pytest.approx(cos(x))
#+end_src

* Appendix: imports
#+begin_src python :tangle no :noweb-ref DIFF_IMPORTS
  from math import log2
  from typing import Callable, Iterator
  from itertools import tee 
  from lazy_utils import repeat_f, within, repeat_itr

  esp = 0.000000001 # a small number that's used to call within()
#+end_src

#+begin_src python :tangle no :noweb-ref TEST_DIFF_IMPORTS
  import pytest
  from itertools import *
  from math import cos, sin

  from lazy_utils import *
  from diff import *
#+end_src
