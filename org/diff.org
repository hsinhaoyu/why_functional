#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
#+EXPORT_FILE_NAME: ../html/diff.html

* Numerical differentiation using iterators

From a glance, this section of Hughes' paper appears to be all about numerical methods that are only of interest to the readers of [[http://numerical.recipes][Numerical Recipes]]. But do read on. This is where lazy evaluation starts to look cool.

** A first stab at the problem
For a real function =f=, the first-order approximation of =f'= at =x= can be obtained by displacing =x= by =h=, and then calculate the slope:

#+begin_src python :noweb yes :tangle ../src/diff.py
  import copy
  from math import log2, sin
  from typing import Callable, Iterator
  from itertools import chain, tee 
  from lazy_utils import repeat_f, within, repeat_itr


  esp = 0.0000000001 # a small number that's used to call within()

  def easydiff(f: Callable[[float], float], x: float) -> float:
      def easydiff_(h: float):
          return (f(x + h) - f(x)) / h

      return easydiff_
#+end_src

Now we generate a lazy list (i.e., iterator) of smaller and smaller =h=, by applying =half= repeatedly with =repeat_f= (defined in =newton.org=). Mapping the slope function to this iterator, =differentiate_= returns another iterator that approximates the derivative closer and closer.

#+begin_src python :noweb yes :tangle ../src/diff.py
  def half(x: float) -> float:
      return x / 2.0

  def differentiate(h0: float, f: Callable[[float], float], x: float) -> Iterator:
      return map(easydiff(f, x), repeat_f(half, h0))
#+end_src

This is our first version of the numerical differentiation function:

#+begin_src python :noweb yes :tangle ../src/diff.py
  def diff1(h0: float, f: Callable[[float], float], x: float) -> float:
      d = within(esp, differentiate(h0, f, x))
      return next(d)
#+end_src

A simple test: differentiate sin(x) at 1.0, starting with a very large gap (=h0=5.0=). We know that the right answer is cos(1.0).

#+begin_src python :noweb yes :tangle ../src/test_diff.py
  import pytest
  from itertools import *
  from math import cos

  from lazy_utils import *
  from diff import *

  def f(x):
      return sin(x)

  def test_diff1():
      x = 1.0
      h0 = 5.0
      d = diff1(h0, f, x)
      assert d == pytest.approx(cos(x))
#+end_src

** Improving convergence with lazy recursive functions
A problem with =differentiate= above is that the sequence converges very slowly. This sections uses numerical tricks to improve the convergence. The math is not our concern here. We will focus just on code structure. 

With some math, it can be shown that with an appropriately chosen integer =n=, convergence can improved with the =elimerror= function defined below. What it does is that it slides a two-item window over the original sequence, and applies a correction (=c= below). 

To accomplish this, we need to add something to the beginning of an iterator, which is done with =itertools.chain= in Python. The following Python code is still short and sweet, but the original Miranda code is much nicer. This is because Miranda evaluates lazily by default, so there is no need for =yield=:

#+begin_src python :noweb yes :tangle ../src/diff.py
  def elimerror__(n: int, itr: Iterator) -> Iterator:
      a, b = next(itr), next(itr)
      p: float = 2.0 ** n
      c: float = (b * p - a) / (p - 1.0)

      for x in chain([c], elimerror(n, chain([b], itr))):
          yield x
#+end_src

#+begin_src python :noweb yes :tangle ../src/diff.py
  def elimerror(n, itr):
      a = next(itr)
      while True:
          b = next(itr)
          p = 2.0 ** n
          c = (b * p - a) / (p - 1.0)
          yield c
          a = b
#+end_src

We don't know how to set =n= yet, but let's try the code with an arbitrary number (=n=2.0=). 

#+begin_src python :noweb yes :tangle ../src/test_diff.py :results output
  def test_elimerror():
      print("\n## test_elimerror:")

      seq1 = differentiate(5.0, f, 1.0)
      print("seq1:", list(islice(seq1, 20)))

      seq1 = differentiate(5.0, f, 1.0)     
      seq2 = elimerror(2.0, seq1)

      print("seq2:", list(islice(seq2, 20)))
#+end_src

The appropriate =n= can be estimated by =order=, using the first three values in the sequence. Taking a slow converging iterator as input, =improve= returns a new iterator that converges much faster. It does it by estimating the order and calling =elimerror=.

#+begin_src python :noweb yes :tangle ../src/diff.py
  def order(itr: Iterator) -> int:
      a, b, c = next(itr), next(itr), next(itr)
      return round(log2((a - c) / (b - c) - 1.0))

  def improve(itr: Iterator) -> Iterator:
      (itr1, itr2) = tee(itr)
      n: int = order(itr1)
      return elimerror(n, itr2)
#+end_src

If you run =make test=, you can see that =improve(seq1)= converges to the correct answer 0.6  much faster.

#+begin_src python :noweb yes :tangle ../src/test_diff.py
  def test_improve():
      print("\n## test_improve")

      seq1 = differentiate(2.0, f, 0.3)
      print("seq1:", list(islice(seq1, 20)))

      seq1 = differentiate(2.0, f, 0.3)
      seq2 = improve(seq1)

      print("seq2:", list(islice(seq2, 20)))
#+end_src

This is the improved differentiation function. The logic is very clear: generate a series of rough calculations, improve the them, and stop when the convergence criterion is met. 

#+begin_src python :noweb yes :tangle ../src/diff.py
  def diff2(h0: float, f: Callable[[float], float], x: float) -> float:
      d = within(esp, improve(differentiate(h0, f, x)))
      return next(d)
#+end_src

A simple test:

#+begin_src python :noweb yes :tangle ../src/test_diff.py
  def test_diff2():
      h0 = 1.0
      x = 0.3
      d = diff2(h0, f, x)
      assert d == pytest.approx(cos(x))
#+end_src

** Super improvements

Do we actually need any improvements? Not for the simple differentiation problem that we've been doing, but this is where the code gets really interesting!

In Hughes' paper, he applied the =improve= function again and again on a list to get better and better convergence. Let =s= be the infinite iterator returned by =differentiate(f0, f x)=. By calling =repeat(improve, s)=, we get =s=, =improve(s)=, =improve(improve(s))=... and so on. It's an infinite iterator of infinite iterators!

The Miranda code in the paper expresses this idea with a beautiful one-liner. Unfortunately, Python's iterator is not as elegant. The =repeat_f= function defined previously (in =newton.org=) doesn't work on iterators, so we'll need a specialized version to take care of the semantics of iterators (without calling =itertools.tee=, all the yielded iterators will be dependent on the first one).

#+begin_src python :noweb yes :tangle ../src/lazy_utils.py
  def repeat_itr(f, i):
      acc = i

      while True:
          (i0, i1) = tee(acc)
          yield i0
          acc = f(i1)
#+end_src

Let's see if the 5th item in the yield iterator is the same as applying =improve= 4 times:

#+begin_src python :noweb yes :tangle ../src/test_diff.py
  def test_repeat_improve():
      print("\n##test_repeat_improve():")
      def f(x):
          return sin(x)

      d = differentiate(1.0, f, 0.3)
      d4 = improve(improve(improve(improve(d))))
      seq1 = list(islice(d4, 5))
      print("seq1:", seq1)

      d = differentiate(1.0, f, 0.3)
      dx = repeat_itr(improve, d)
      next(dx)
      next(dx)
      next(dx)
      next(dx)
      seq2 = list(islice(next(dx), 5))
      print("seq2:", seq2)

      assert seq1 == seq2
#+end_src

I don't know enough numerical analysis to understand why this works, but the paper says that the
second approximation from each of the improved iterator is a good sequence for solving our differentiation problem. So, we have our third version:

#+begin_src python :noweb yes :tangle ../src/diff.py
  def second(itr):
      next(itr)
      return next(itr)

  def super_improve(itr):
      return map(second, repeat_itr(improve, itr))

  def diff3(h0: float, f: Callable[[float], float], x: float) -> float:
      d = within(esp, super_improve(differentiate(h0, f, x)))
      return next(d)
#+end_src

Does it work?

#+begin_src python :noweb yes :tangle ../src/test_diff.py
  def test_diff3():
      h0 = 1.0
      x = 0.3
      d = diff3(h0, f, x)
      assert d == pytest.approx(cos(x))
#+end_src
