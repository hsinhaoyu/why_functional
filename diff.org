* Numerical differentiation using iterators

From a glance, this section of Hughes' paper appears to be all about numerical methods that are only of interest to readers of  [[http://numerical.recipes][Numerical Recipes]]. But do read on. It has some very elegant code.

** A first stab at the problem

For function =f=, the first-order approximation of =f'(x)= is the slope at =x=, calculated with a small displacement =h=.

=differentiate_()=: To create a lazy list (i.e., iterator) of smaller and smaller =h=, we apply the =half= function repeatedly using =repeat=, defined in =newton.org=. Then, apply =easydiff= using =map=.

We iterate over this lazy list, until the change becomes very small. This is done in =differentiate=, using the =within= function defined in =newton.org=.

#+begin_src python :noweb yes :tangle src/diff.py
  from typing import Callable, Iterator
  from itertools import chain 
  from lazy_utils import *

  def easydiff(f: Callable[[float], float], x: float) -> float:
      def easydiff_(h: float):
          return (f(x + h) - f(x)) / h

      return easydiff_

  def half(x: float) -> float:
      return x / 2.0

  def differentiate_(h0: float, f: Callable[[float], float], x: float) -> Iterator:
      return map(easydiff(f, x), repeat(half, h0))

  def differentiate(h0: float, f: Callable[[float], float], x: float) -> Iterator:
      d = differentiate_(h0, f, x)
      return next(within(0.000000001, d))
#+end_src

A simple test: differentiate x^2 at 1.0.

#+begin_src python :noweb yes :tangle src/test_diff.py
  import pytest
  from itertools import *

  from diff import *

  def f(x):
      return x * x

  def test_differentiate():
      a = differentiate(5.0, f, 1.0)
      assert a == pytest.approx(2.0)
#+end_src

** Improving the efficiency with more higher order functions
A problem with =differentiate_= above is that the sequence converges very slowly. This sections uses numerical tricks to improve the convergence. The math is not our concern here. We will focus just on the iteration.

With some math, it can be shown that with an appropriately chosen integer =n=, convergence can be accelerated with the =elimerror= function defined below. What it does is that it slides a two-item window over the original sequence, and applies a correction (=c= below). 

To accomplish this, we need to add something to the beginning of an iterator. In Python, we can use =itertools.chain=. The following Python code is still short and sweet, but the original Miranda code is much nicer. This is because Miranda evaluates everything lazily.

#+begin_src python :noweb yes :tangle src/diff.py
  def elimerror(n: float, itr: Iterator) -> Iterator:
      a, b = next(itr), next(itr)
      p: float = 2.0 ** n
      c: float = (b * p - a) / (p - 1)

      for x in chain([c], elimerror(n, chain([b], itr))):
          yield x
#+end_src

We don't know how to set =n= yet, but let's try the code with an arbitrary number (=n=2.0).
#+begin_src python :noweb yes :tangle src/test_diff.py
  def test_elimerror():
      print("\n## test_elimerror\n")

      def f(x):
          return x * x

      seq1 = differentiate_(5.0, f, 1.0)
      print("seq1:", list(islice(seq1, 5)))      

      seq1 = differentiate_(5.0, f, 1.0)      
      seq2 = elimerror(2.0, seq1)

      print("seq2:", list(islice(seq2, 5)))
#+end_src
